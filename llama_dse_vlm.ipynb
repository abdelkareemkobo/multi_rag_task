{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f093833",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import torch\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from qdrant_client import QdrantClient, models\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"embedding_model_name\": \"llamaindex/vdr-2b-multi-v1\",\n",
    "    \"vlm_model_name\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    \"vlm_processor_name\": \"Qwen/Qwen2.5-VL-7B-Instruct\",  # Corrected from -AWQ\n",
    "    \"data_dir\": \"./data_images\",\n",
    "    \"collection_name\": \"llama-multi\",\n",
    "    \"device_embedding\": \"cuda:1\",\n",
    "    \"device_vlm\": \"cuda:0\",\n",
    "    \"image_extensions\": [\".jpg\", \".png\"],\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"image_resize\": (850, 850)\n",
    "}\n",
    "\n",
    "\n",
    "# Ground Truth for Retrieval\n",
    "GROUND_TRUTH = {\n",
    "    \"How is the scaled dot-product attention calculated?\": [\"page4.png\"],\n",
    "    \"What is the BLEU score of the model in English to German translation EN-DE?\": [\"page8.png\"],\n",
    "    \"How long were the base and big models trained?\": [\"page7.png\"],\n",
    "    \"Which optimizer was used when training the models?\": [\"page7.png\"],\n",
    "    \"Show me a picture that shows the difference between Scaled Dot-Product Attention and Multi-Head Attention.\": [\"page4.png\"],\n",
    "    \"similar.png\": [\"page4.png\"]\n",
    "}\n",
    "\n",
    "# Reference Answers for Generation\n",
    "REFERENCES = {\n",
    "    \"How is the scaled dot-product attention calculated?\": (\n",
    "        \"Scaled dot-product attention is calculated as follows: Given queries (Q), keys (K), and values (V) matrices \"\n",
    "        \"of dimensions d_k and d_v, compute the dot product of Q and K, divide by the square root of d_k, apply a softmax \"\n",
    "        \"function to obtain attention weights, and multiply by V. The formula is: Attention(Q, K, V) = softmax(QK^T / √d_k)V.\"\n",
    "    ),\n",
    "    \"What is the BLEU score of the model in English to German translation EN-DE?\": (\n",
    "        \"The Transformer base model achieves a BLEU score of 27.3, and the big model achieves a BLEU score of 28.4 \"\n",
    "        \"on the WMT 2014 English-to-German translation task.\"\n",
    "    ),\n",
    "    \"How long were the base and big models trained?\": (\n",
    "        \"The base Transformer models were trained for 100,000 steps, approximately 12 hours, on 8 NVIDIA P100 GPUs. \"\n",
    "        \"The big Transformer models were trained for 300,000 steps, approximately 3.5 days.\"\n",
    "    ),\n",
    "    \"Which optimizer was used when training the models?\": (\n",
    "        \"The Transformer models were trained using the Adam optimizer with β1 = 0.9, β2 = 0.98, and ε = 10^-9.\"\n",
    "    ),\n",
    "    \"Show me a picture that shows the difference between Scaled Dot-Product Attention and Multi-Head Attention.\": (\n",
    "        \"Figure 2 in the Transformer paper illustrates the difference: Scaled Dot-Product Attention computes attention \"\n",
    "        \"using a single query-key-value operation scaled by √d_k, while Multi-Head Attention performs multiple such operations \"\n",
    "        \"in parallel, projecting queries, keys, and values into h subspaces, concatenating the results, and applying a final projection.\"\n",
    "    ),\n",
    "    \"What is the name of the attention function of this image?\": (\n",
    "        \"The image depicts Scaled Dot-Product Attention and Multi-Head Attention.\"\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9af8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_embedding_model() -> HuggingFaceEmbedding:\n",
    "    \"\"\"Initialize the VDRE embedding model.\"\"\"\n",
    "    return HuggingFaceEmbedding(\n",
    "        model_name=CONFIG[\"embedding_model_name\"],\n",
    "        device=CONFIG[\"device_embedding\"],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "def initialize_vlm_model() -> tuple[Qwen2_5_VLForConditionalGeneration, AutoProcessor]:\n",
    "    \"\"\"Initialize the Qwen2.5-VL model and processor.\"\"\"\n",
    "    vlm = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        CONFIG[\"vlm_model_name\"],\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=CONFIG[\"device_vlm\"]\n",
    "    )\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        CONFIG[\"vlm_processor_name\"],\n",
    "        use_fast=True\n",
    "    )\n",
    "    return vlm, processor\n",
    "\n",
    "def initialize_sbert_model() -> SentenceTransformer:\n",
    "    return SentenceTransformer('paraphrase-MiniLM-L12-v2')\n",
    "\n",
    "def load_documents(data_dir: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load image documents from the data directory.\"\"\"\n",
    "    return [\n",
    "        {\"caption\": filename, \"image\": os.path.join(data_dir, filename)}\n",
    "        for filename in os.listdir(data_dir)\n",
    "        if any(filename.endswith(ext) for ext in CONFIG[\"image_extensions\"])\n",
    "    ]\n",
    "\n",
    "def generate_embeddings(\n",
    "    model: HuggingFaceEmbedding,\n",
    "    documents: List[Dict[str, str]]\n",
    ") -> tuple[List, List]:\n",
    "    \"\"\"Generate text and image embeddings for documents.\"\"\"\n",
    "    text_embeddings = model.get_text_embedding_batch([doc[\"caption\"] for doc in documents])\n",
    "    image_embeddings = model.get_image_embedding_batch([doc[\"image\"] for doc in documents])\n",
    "    return text_embeddings, image_embeddings\n",
    "\n",
    "def initialize_vector_store() -> QdrantClient:\n",
    "    \"\"\"Initialize the Qdrant vector store client.\"\"\"\n",
    "    return QdrantClient(\":memory:\")\n",
    "\n",
    "def setup_collection(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    text_embeddings: List,\n",
    "    image_embeddings: List\n",
    ") -> None:\n",
    "    \"\"\"Set up the Qdrant collection if it doesn't exist.\"\"\"\n",
    "    if not client.collection_exists(collection_name):\n",
    "        client.create_collection(\n",
    "            collection_name=collection_name,\n",
    "            vectors_config={\n",
    "                \"image\": models.VectorParams(size=len(image_embeddings[0]), distance=models.Distance.COSINE),\n",
    "                \"text\": models.VectorParams(size=len(text_embeddings[0]), distance=models.Distance.COSINE)\n",
    "            }\n",
    "        )\n",
    "\n",
    "def upload_documents(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    documents: List[Dict[str, str]],\n",
    "    text_embeddings: List,\n",
    "    image_embeddings: List\n",
    ") -> None:\n",
    "    \"\"\"Upload documents and their embeddings to the vector store.\"\"\"\n",
    "    client.upload_points(\n",
    "        collection_name=collection_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=idx,\n",
    "                vector={\"text\": text_embeddings[idx], \"image\": image_embeddings[idx]},\n",
    "                payload=doc\n",
    "            )\n",
    "            for idx, doc in enumerate(documents)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def query_image(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    model: HuggingFaceEmbedding,\n",
    "    query: str | Image.Image,\n",
    "    vector_type: str = \"image\",\n",
    "    limit: int = 1,\n",
    "    resize: bool = False\n",
    ") -> Any:\n",
    "    \"\"\"Query the vector store for images based on text or image input.\"\"\"\n",
    "    query_vector = (\n",
    "        model.get_query_embedding(query)\n",
    "        if isinstance(query, str)\n",
    "        else model.get_image_embedding(query)\n",
    "    )\n",
    "    result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        using=vector_type,\n",
    "        with_payload=[\"image\", \"caption\"],\n",
    "        limit=limit\n",
    "    ).points\n",
    "    if limit == 1:\n",
    "        result = result[0]\n",
    "        if resize:\n",
    "            result.payload[\"image\"] = Image.open(result.payload[\"image\"]).resize(CONFIG[\"image_resize\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_vlm_query(\n",
    "    vlm: Qwen2_5_VLForConditionalGeneration,\n",
    "    processor: AutoProcessor,\n",
    "    image: Image.Image,\n",
    "    query: str\n",
    ") -> str:\n",
    "    \"\"\"Process a query with the vision-language model.\"\"\"\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": query}\n",
    "        ]\n",
    "    }]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(CONFIG[\"device_vlm\"])\n",
    "    generated_ids = vlm.generate(**inputs, max_new_tokens=CONFIG[\"max_new_tokens\"])\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    return processor.batch_decode(\n",
    "        generated_ids_trimmed,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_retrieval(\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    model: HuggingFaceEmbedding,\n",
    "    queries: List[Any],\n",
    "    ground_truth: Dict[str, List[str]],\n",
    "    image_query_path: str,\n",
    "    k: int = 1\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate retrieval performance for queries.\"\"\"\n",
    "    precisions, recalls = [], []\n",
    "    for query in queries:\n",
    "        query_key = os.path.basename(image_query_path) if not isinstance(query, str) else query\n",
    "        result = query_image(client, collection_name, model, query, vector_type=\"image\", limit=k)\n",
    "        retrieved_images = [result.payload[\"image\"]] if k == 1 else [point.payload[\"image\"] for point in result]\n",
    "        relevant_images = ground_truth.get(query_key, [])\n",
    "        y_true = [1 if os.path.basename(img) in relevant_images else 0 for img in retrieved_images]\n",
    "        y_pred = [1] * len(retrieved_images)\n",
    "        precisions.append(precision_score(y_true, y_pred, zero_division=0))\n",
    "        recalls.append(recall_score(y_true, y_pred, zero_division=0) if relevant_images else 0)\n",
    "    return {\n",
    "        f\"Precision@{k}\": sum(precisions) / len(precisions) if precisions else 0,\n",
    "        f\"Recall@{k}\": sum(recalls) / len(recalls) if recalls else 0\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_generation(\n",
    "    vlm: Qwen2_5_VLForConditionalGeneration,\n",
    "    processor: AutoProcessor,\n",
    "    client: QdrantClient,\n",
    "    collection_name: str,\n",
    "    model: HuggingFaceEmbedding,\n",
    "    queries: List[Any],\n",
    "    vlm_queries: List[str],\n",
    "    references: Dict[str, str],\n",
    "    sbert_model: SentenceTransformer\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate VLM generation performance.\"\"\"\n",
    "    bleu_scores, sbert_scores = [], []\n",
    "    from nltk.translate.bleu_score import SmoothingFunction\n",
    "    smoothie = SmoothingFunction().method4  # Use method4 for robustness\n",
    "    for query, vlm_query in zip(queries, vlm_queries):\n",
    "        result = query_image(client, collection_name, model, query, vector_type=\"image\", limit=1)\n",
    "        image = Image.open(result.payload[\"image\"])\n",
    "        generated_text = process_vlm_query(vlm, processor, image, vlm_query)\n",
    "        reference = references.get(vlm_query, \"\")\n",
    "        if generated_text.strip() and reference.strip():  # Ensure non-empty texts\n",
    "            # BLEU score with smoothing\n",
    "            bleu_scores.append(sentence_bleu(\n",
    "                [reference.split()], \n",
    "                generated_text.split(), \n",
    "                weights=(0.5, 0.5, 0, 0),\n",
    "                smoothing_function=smoothie\n",
    "            ))\n",
    "            # SBERT score\n",
    "            embeddings = sbert_model.encode([generated_text, reference], convert_to_tensor=True)\n",
    "            sbert_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "            sbert_scores.append(sbert_score)\n",
    "    return {\n",
    "        \"Average BLEU\": sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0,\n",
    "        \"Average SBERT Score\": sum(sbert_scores) / len(sbert_scores) if sbert_scores else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719bdfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = initialize_embedding_model()\n",
    "vlm, vlm_processor = initialize_vlm_model()\n",
    "sbert_model = initialize_sbert_model()\n",
    "documents = load_documents(CONFIG[\"data_dir\"])\n",
    "\n",
    "text_embeddings, image_embeddings = generate_embeddings(embedding_model, documents)\n",
    "\n",
    "client = initialize_vector_store()\n",
    "setup_collection(client, CONFIG[\"collection_name\"], text_embeddings, image_embeddings)\n",
    "upload_documents(client, CONFIG[\"collection_name\"], documents, text_embeddings, image_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2108f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Query: How is the scaled dot-product attention calculated?\n",
      "VLM Output: The scaled dot-product attention is calculated by first computing the dot products of the query with all keys, dividing each by \\(\\sqrt{d_k}\\), and then applying a softmax function to obtain the weights on the values. The formula for this is given by:\n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
      "\\]\n",
      "\n",
      "Here:\n",
      "- \\(Q\\) is the query matrix.\n",
      "- \\(K\\) is the key matrix.\n",
      "- \\(V\\) is the value matrix.\n",
      "- \\(d_k\\) is the dimensionality of the keys and queries.\n",
      "- \\(\\sqrt{d_k}\\) is used to scale the dot products to prevent the softmax function from having extremely small gradients when \\(d_k\\) is large.\n",
      "\n",
      "The softmax function ensures that the weights sum to 1, making them valid probabilities. These weights are then applied to the values to compute the final output of the attention mechanism.\n",
      "\n",
      "Text Query: What is the BLEU score of the model in English to German translation EN-DE?\n",
      "VLM Output: The BLEU score of the model in English to German translation (EN-DE) is 27.3.\n",
      "\n",
      "Text Query: How long were the base and big models trained?\n",
      "VLM Output: The base models were trained for 100,000 steps or 12 hours, while the big models were trained for 300,000 steps (3.5 days).\n",
      "\n",
      "Text Query: Which optimizer was used when training the models?\n",
      "VLM Output: The Adam optimizer was used when training the models. The specific parameters for the Adam optimizer were \\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.98\\), and \\(\\epsilon = 10^{-9}\\).\n",
      "\n",
      "Text Query: Show me a picture that shows the difference between Scaled Dot-Product Attention and Multi-Head Attention.\n",
      "VLM Output: The image you provided already shows the difference between Scaled Dot-Product Attention and Multi-Head Attention in Figure 2. Here's a brief explanation based on the figure:\n",
      "\n",
      "1. **Scaled Dot-Product Attention (left side of Figure 2):**\n",
      "   - This is a single attention mechanism.\n",
      "   - It takes queries (Q), keys (K), and values (V) as inputs.\n",
      "   - The process involves:\n",
      "     - Computing the dot product of the query with all keys.\n",
      "     - Dividing each dot product by \\(\\sqrt{d_k}\\) to scale the attention scores.\n",
      "     - Applying a softmax function to get the weights on the values.\n",
      "     - Multiplying the weighted values by the weights to get the final output.\n",
      "\n",
      "2. **Multi-Head Attention (right side of Figure 2):**\n",
      "   - This is an extension of the Scaled Dot-Product Attention, where multiple attention heads are used in parallel.\n",
      "   - Each head computes its own attention scores independently.\n",
      "   - The outputs from each head are concatenated and then passed through a linear layer to produce the final output.\n",
      "   - This allows for more complex and diverse attention patterns across different parts of the input sequence.\n",
      "\n",
      "The figure visually represents these differences, showing how Multi-Head Attention combines multiple Scaled Dot-Product Attention mechanisms in parallel.\n",
      "\n",
      "Image Query: ./images/similar.png\n",
      "VLM Query: What is the name of the attention function of this image?\n",
      "VLM Output: The attention function shown in the left part of Figure 2 is called \"Scaled Dot-Product Attention.\"\n",
      "\n",
      "Retrieval Metrics: {'Precision@1': 1.0, 'Recall@1': 1.0}\n",
      "Generation Metrics: {'Average BLEU': 0.15422318189920772, 'Average SBERT Score': 0.7881203691164652}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_queries = [\n",
    "    \"How is the scaled dot-product attention calculated?\",\n",
    "    \"What is the BLEU score of the model in English to German translation EN-DE?\",\n",
    "    \"How long were the base and big models trained?\",\n",
    "    \"Which optimizer was used when training the models?\",\n",
    "    \"Show me a picture that shows the difference between Scaled Dot-Product Attention and Multi-Head Attention.\"\n",
    "]\n",
    "image_query_path = \"./images/similar.png\"\n",
    "queries = text_queries + [Image.open(image_query_path) if os.path.exists(image_query_path) else None]\n",
    "vlm_queries = text_queries + [\"What is the name of the attention function of this image?\"]  # VLM query for image\n",
    "\n",
    "for query, vlm_query in zip(queries, vlm_queries):\n",
    "    if query is None:\n",
    "        print(f\"Image Query Skipped: {image_query_path} not found\\n\")\n",
    "        continue\n",
    "    result = query_image(client, CONFIG[\"collection_name\"], embedding_model, query, resize=False)\n",
    "    image = Image.open(result.payload[\"image\"])\n",
    "    output = process_vlm_query(vlm, vlm_processor, image, vlm_query)\n",
    "    if isinstance(query, str):\n",
    "        print(f\"Text Query: {query}\\nVLM Output: {output}\\n\")\n",
    "    else:\n",
    "        print(f\"Image Query: {image_query_path}\\nVLM Query: {vlm_query}\\nVLM Output: {output}\\n\")\n",
    "\n",
    "retrieval_metrics = evaluate_retrieval(\n",
    "    client, CONFIG[\"collection_name\"], embedding_model, [q for q in queries if q is not None], GROUND_TRUTH, image_query_path, k=1\n",
    ")\n",
    "generation_metrics = evaluate_generation(\n",
    "    vlm, vlm_processor, client, CONFIG[\"collection_name\"], embedding_model,\n",
    "    [q for q in queries if q is not None], vlm_queries, REFERENCES, sbert_model\n",
    ")\n",
    "print(\"Retrieval Metrics:\", retrieval_metrics)\n",
    "print(\"Generation Metrics:\", generation_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f11b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae82506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f812f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xbites",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
